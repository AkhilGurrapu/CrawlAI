{"https://docs.snowflake.com/": {"title": "Snowflake Documentation", "content": "Basic information and instructions for first-time users of Snowflake.\nInstructions on performing various Snowflake operations.\nWrite applications that extend Snowflake, act as a client, or act as an integrating component.\nReference for SQL data types, SQL commands, SQL functions, SQL classes, scripting, views, and other areas\nTutorials to help you learn the basics of using Snowflake\nOverview of the new features, enhancements, and important fixes introduced in the most recent releases of Snowflake.", "language": "en"}, "https://docs.snowflake.com/en/guides-overview-connecting": {"title": "Applications and tools for connecting to Snowflake | Snowflake Documentation", "content": "Applications and tools for connecting to Snowflake | Snowflake Documentation\nApplications and tools for connecting to Snowflake\n\u00b6\nSnowflake provides several different applications and tools that you can use to access databases in Snowflake.\nNote\nFor information about configuring clients, driver, libraries, and third-party applications to connect to Snowflake, see\nConfiguring a client, driver, library, or third-party application to connect to Snowflake\n.\nUser interfaces\n\u00b6\nSnowsight: The Snowflake web interface\nSnowsight distills Snowflake\u2019s powerful SQL support into a unified, easy-to-use experience.\nUse Snowsight to perform your critical Snowflake operations.\nClassic Console\nUse the Classic Console to perform tasks that would normally performed using SQL and the command line.\nCommand-line clients\n\u00b6\nSnowflake CLI\nUse the command line to create, manage, update, and view apps running on Snowflake across workloads.\nSnowSQL (CLI client)\nDetailed instructions for installing, configuring, and using the Snowflake command-line client.\nExtensions for code editors\n\u00b6\nSnowflake Extension for Visual Studio Code\nUse the Snowflake Extension for Visual Studio Code to connect to Snowflake within Visual Studio Code and perform SQL operations.\nInfrastructure as code\n\u00b6\nNote\nThe following content is not supported by Snowflake. All code is provided \u201cAS IS\u201d and without warranty.\nSnowflake Terraform provider\nDocumentation and resources for the Snowflake Terraform provider.\nDrivers and libraries\n\u00b6\nAPI Reference\nLists the drivers and APIs provided by Snowflake for writing applications that connect to Snowflake.\nIntegrating with third-party systems\n\u00b6\nSnowflake Connectors\nSnowflake Connectors allow you to integrate third-party applications and database systems with Snowflake.\nThird-party software\n\u00b6\nSnowflake Ecosystem\nOverview of the third-party tools and technologies, as well as the Snowflake-provided clients, in the Snowflake ecosystem.\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/user-guide/warehouses": {"title": "Virtual warehouses | Snowflake Documentation", "content": "Virtual warehouses | Snowflake Documentation\nVirtual warehouses\n\u00b6\nA virtual warehouse, often referred to simply as a \u201cwarehouse\u201d, is a cluster of compute resources in Snowflake. A virtual warehouse is\navailable in two types:\nStandard\nSnowpark-optimized\nA warehouse provides the required resources, such as CPU, memory, and temporary storage, to\nperform the following operations in a Snowflake session:\nExecuting SQL\nSELECT\nstatements that require compute resources (e.g. retrieving rows from tables and views).\nPerforming DML operations, such as:\nUpdating rows in tables (\nDELETE\n,\nINSERT\n,\nUPDATE\n).\nLoading data into tables (\nCOPY INTO <table>\n).\nUnloading data from tables\n(\nCOPY INTO <location>\n).\nNote\nTo perform these operations, a warehouse must be running and in use for the session. While a warehouse is running, it consumes Snowflake\ncredits.\nOverview of warehouses\nWarehouses are required for queries, as well as all DML operations, including loading data into tables.\nIn addition to being defined by its type as either Standard or Snowpark-optimized, a warehouse is defined by its size,\nas well as the other properties that can be set to help control and automate warehouse activity.\nSnowpark-optimized warehouses\nSnowpark workloads can be run on both Standard and Snowpark-optimized warehouses. Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases\nWarehouse considerations\nBest practices and general guidelines for using virtual warehouses in Snowflake to process queries\nMulti-cluster warehouses\nMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.\nWorking with warehouses\nLearn how to create, stop, start and otherwise manage Snowflake warehouses.\nUsing the Query Acceleration Service\nThe query acceleration service can accelerate parts of the query workload in a warehouse.\nWhen enabled for a warehouse, query acceleration can improve overall warehouse performance by reducing the impact of outlier queries\n(i.e. queries which use more resources then typical queries).\nMonitoring warehouse load\nWarehouse query load measures the average number of queries that were running or queued within a specific interval.\nOverview of warehouses\nSnowpark-optimized warehouses\nWarehouse considerations\nMulti-cluster warehouses\nWorking with warehouses\nUsing the Query Acceleration Service\nMonitoring warehouse load\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/guides-overview-db": {"title": "Databases, Tables and Views - Overview | Snowflake Documentation", "content": "Databases, Tables and Views - Overview | Snowflake Documentation\nDatabases, Tables and Views - Overview\n\u00b6\nAll data in Snowflake is maintained in databases. Each database consists of one or more schemas, which are logical groupings of database objects,\nsuch as tables and views. Snowflake does not place any hard limits on the number of databases, schemas (within a database), or objects (within\na schema) you can create.\nUse the following pages to learn about tables and table types, views, design considerations and other related content.\nUnderstanding Snowflake Table Structures\nIntroduction to\nmicro-partitions\nand\ndata clustering\n, two of the principal concepts utilized in Snowflake physical table structures.\nTemporary and Transient Tables\nSnowflake supports creating temporary tables for storing non-permanent, transitory data such as ETL data, session-specific\nor other short lived data.\nExternal Tables\nSnowflake supports the concept of an external table. External tables are read-only, and their files are stored in an external stage.\nHybrid Tables\nSnowflake supports the concept of a hybrid table. Hybrid tables provide\noptimized performance for read and write operations in transactional and\nhybrid workloads.\nApache Iceberg\u2122 tables\nSnowflake supports the Apache Iceberg\u2122 open table format. Iceberg tables use data in external cloud\nstorage and give you the option to use Snowflake as the Iceberg catalog, an external Iceberg catalog, or to create a table\nfrom files in object storage.\nViews\nA view allows the result of a query to be accessed as if it were a table.\nViews serve a variety of purposes, including combining, segregating, and protecting data.\nSecure Views\nSnowflake supports the concept of a secure view. Secure views are specifically designed for data privacy.\nFor example to limit access to sensitive data that should not be exposed to all users of the underlying table(s).\nMaterialized Views\nMaterialized views are views precomputed from data derived from a query specification and stored for later use.\nQuerying a materialized view is faster than executing a query against the base table of the view because the data is pre-computed.\nTable Design Best Practices\nBest practices, general guidelines, and important considerations when designing and managing tables.\nCloning Best Practices\nBest practices, general guidelines, and important considerations when cloning objects in Snowflake, particularly databases, schemas,\nand permanent tables.\nData storage considerations\nBest practices and guidelines for controlling data storage costs associated with Continuous Data Protection (CDP), particularly for tables.\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/guides-overview-loading-data": {"title": "Load data into Snowflake | Snowflake Documentation", "content": "Load data into Snowflake | Snowflake Documentation\nLoad data into Snowflake\n\u00b6\nData can be loaded into Snowflake in a number of ways.\nThe following topics provide an overview of data loading concepts, tasks, tools, and techniques to quick and easily load data into your Snowflake database.\nOverview of data loading\nOptions available to load data into Snowflake.\nSummary of data loading features\nReference of the supported features for using the\nCOPY INTO <table>\ncommand to load data from files.\nData loading considerations\nBest practices, general guidelines, and important considerations for bulk data loading.\nWorking with Amazon S3-compatible storage\nInstructions for accessing data in other storage.\nLoad data using the web interface\nInstructions for loading limited amounts of data using the web interface.\nIntroduction to Loading Semi-structured Data\nConsiderations for loading semi-structured data.\nIntroduction to unstructured data\nConsiderations for loading unstructured data.\nBulk loading from a local file system\nInstructions for loading data in bulk using the COPY command.\nSnowpipe\nInstructions for loading data continuously using Snowpipe.\nSnowpipe Streaming\nInstructions for loading data streams continuously using Snowpipe Streaming.\nTransforming data during a load\nInstructions for transforming data while loading it into a table using the COPY INTO command.\nQuerying Data in Staged Files\nInstructions on using standard SQL to query internal and external named stages.\nQuerying Metadata for Staged Files\nInstructions on querying metadata in internal and external stages.\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/user-guide/admin-trial-account": {"title": "Trial accounts | Snowflake Documentation", "content": "Trial accounts | Snowflake Documentation\nTrial accounts\n\u00b6\nA Snowflake trial account lets you evaluate/test Snowflake\u2019s full range of innovative and powerful features with no cost or contractual obligations. To sign\nup for a trial account, all you need is a valid email address; no payment information or other qualifying information is required.\nSigning up for a trial account\n\u00b6\nYou can sign up for a free trial using the\nself-service form\n(on the Snowflake website).\nWhen you sign up for a trial account, you select your\ncloud platform\n,\nregion\n,\nand\nSnowflake Edition\n. These selections can affect how quickly you exhaust your free usage balance. For example, some features available in the Enterprise Edition consume additional\ncredits\n.\nThe balance of your free usage decreases as you consume credits to use\ncompute resources\nand accrue costs associated with\nstorage\n. You can\ntrack your remaining balance\nat any time.\nThe trial continues for 30 days (from the sign-up date) or until you\u2019ve depleted your free usage balance, whichever occurs first. At any time during the trial, you can\ncancel the trial\nor\nconvert the account to a paid account\n.\nAt the end of the trial, the account is suspended. You can still log into a suspended account, but you cannot use any features, such as running a virtual warehouse,\nloading data, or performing queries.\nTo reactivate a suspended trial account, you must enter a credit card, which converts it to a paid account.\nUsing compute resources\n\u00b6\nVirtual warehouses\nprovide the compute power to\nload data\nand\nperform queries\n. These warehouses consume credits, which reduces your free usage balance. To begin, simply start a warehouse; any credits consumed by the warehouse will be deducted from your balance. If your credit consumption fully depletes your free usage balance, you must add a credit card to the account to continue using Snowflake.\nFree credits are only consumed by the virtual warehouses you create in your account, and only when they are running.\nTip\nTo prevent unintentional usage of your free credits:\nVerify the size of your virtual warehouses before you start/resume them. The larger the warehouse, the more credits it consumes while running.\nIn many situations, Small or Medium size warehouses are sufficient for evaluating Snowflake\u2019s loading and querying capabilities.\nDo not disable\nauto-suspend\nwhen creating a warehouse. Choosing a short auto-suspend time period (e.g. 5 minutes or less) can reduce credit consumption.\nFor additional tips on using your trial account:\nIn the left navigation bar, find the tile showing your remaining balance.\nSelect\n\u2026\n\u00bb\nUsing your trial credits\n.\nTrial accounts without a valid payment methods are limited to roughly one credit per day of usage of\nSnowflake Cortex LLM functions\n.\nTo remove this restriction,\nconvert your trial account to a paid account\n.\nUsing storage\n\u00b6\nAs you load data into your trial account, the cost of that storage is subtracted from your free usage balance based on the standard On-Demand cost of a TB in your cloud platform and region. In addition to the cost of storage, loading data also consumes credits as it uses the compute resources of a warehouse.\nTutorials for trial accounts\n\u00b6\nThe following tutorials are available for trial accounts:\nCreate users and grant roles\nLoad and query sample data using SQL\nLoad and query sample data using Snowpark Python\nLoad data from cloud storage: Amazon S3\nLoad data from cloud storage: Microsoft Azure\nLoad data from cloud storage: GCS\nTracking your remaining balance\n\u00b6\nUsers with the ACCOUNTADMIN role can track the remaining balance of their trial using a tile in the left navigation bar of\nSnowsight.\nFrom this tile you can also:\nSelect\nUpgrade\nto\nconvert the trial account to a paid account\n.\nSelect\n\u2026\n\u00bb\nsee organization usage details\nto access the\nUsage\npage, which allows you to drill down into your credit\nconsumption and storage costs.\nSelect the\n\u2026\nbutton to access resources that help you get the most out of your trial account.\nConverting to a paid account\n\u00b6\nYou can add a credit card to a trial account at any time to convert it to a paid account.\nSign in to Snowsight.\nDo one of the following:\nSelect\nUpgrade\nin the left navigation.\nSelect\nAdmin\n\u00bb\nBilling & Terms\n.\nSelect\nPayment Methods\n.\nSelect\n+ Credit Card\n.\nEnter the required information and select\nAdd Card\n.\nWhen you enter a credit card, Snowflake verifies that the card is valid by charging $1 (USD). No other charges are billed at that time.\nNote that you can also change the credit card for a trial account, at any time, using the same interface in which you added the card. Each time you\nenter a new credit card, the new card will be charged $1 (USD).\nNote\nAdding a credit card to a trial account converts it to a paid account without ending the trial period. During the remainder of the trial period, you can continue using your free credits and storage until the balance is exhausted, after which all additional credit consumption and storage costs will be charged.\nUnused balances expire when the trial period ends, at which time costs (for consuming credits and storing data) are charged to the credit card on file at the end of each billing cycle (typically monthly).\nFor pricing details, see the\npricing page\n(on the Snowflake website).\nCanceling a trial account\n\u00b6\nYou can cancel a trial account at any time by contacting\nSnowflake Support\nand requesting the account to be canceled.\nNote\nCurrently, trial accounts cannot be canceled through the web interface. To cancel an account, you must contact\nSnowflake Support\n.\nCurrent limitations for trial accounts\n\u00b6\nThe following features are not available for trial accounts:\nExternal network access\nHybrid tables\nOutbound private connectivity\nSnowpark Container Services\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/user-guide/getting-started-tutorial": {"title": "Snowflake in 20 minutes | Snowflake Documentation", "content": "Snowflake in 20 minutes | Snowflake Documentation\nSnowflake in 20 minutes\n\u00b6\nIntroduction\n\u00b6\nThis tutorial uses the Snowflake command line client,\nSnowSQL\n, to introduce key concepts and tasks, including:\nCreating Snowflake objects\u2014You create a database and a table for storing data.\nLoading data\u2014We provide small sample CSV data files for you to load into the table.\nQuerying\u2014You explore sample queries.\nNote\nSnowflake bills a minimal amount for the on-disk storage used for the sample data in\nthis tutorial. The tutorial provides steps to drop the database and minimize storage\ncost.\nSnowflake requires a\nvirtual warehouse\nto load the\ndata and execute queries. A running virtual warehouse consumes Snowflake credits.\nIn this tutorial, you will be using a\n30-day trial account\n,\nwhich provides free credits, so you won\u2019t incur any costs.\nWhat you\u2019ll learn\n\u00b6\nIn this tutorial you\u2019ll learn how to:\nCreate Snowflake objects\u2014You create a database and a table for storing data.\nInstall SnowSQL\u2014You install and use SnowSQL, the Snowflake command line query tool.\nUsers of Visual Studio Code might consider using the\nSnowflake Extension for Visual Studio Code\ninstead of SnowSQL.\nLoad CSV data files\u2014You use various mechanisms to load data into tables from CSV files.\nWrite and execute sample queries\u2014You write and execute a variety of queries against newly loaded data.\nPrerequisites\n\u00b6\nThis tutorial requires a database, table, and virtual warehouse to load and query data.\nCreating these Snowflake objects requires a Snowflake user with a role with the\nnecessary access control privileges. In addition,\nSnowSQL\nis required to execute the SQL statements in the tutorial. Lastly, the tutorial requires CSV files that contain sample data to load.\nYou can complete this tutorial using an existing Snowflake warehouse, database, and table, and your own local data files, but we recommend using the Snowflake objects and the set of\nprovided data.\nTo set up Snowflake for this tutorial, complete the following before continuing:\nCreate a user\nTo create the database, table, and virtual warehouse, you must be logged in as a\nSnowflake user with a role that grants you the privileges to create these objects.\nIf you\u2019re using a 30-day trial account, you can log in as the user that was created for the account.\nThis user has the role with the privileges needed to create the objects.\nIf you don\u2019t have a Snowflake user, you can\u2019t perform this tutorial.\nIf you don\u2019t have a role that lets you create a user, ask someone who does to perform this step for you.\nUsers with the ACCOUNTADMIN or SECURITYADMIN role can create users.\nInstall SnowSQL\nTo install SnowSQL, see\nInstalling SnowSQL\n.\nDownload sample data files\nFor this tutorial you download sample employee data files in CSV format that Snowflake provides.\nTo download and unzip the sample data files:\nDownload the set of sample data files. Right-click the name of the archive\nfile,\ngetting-started.zip\n, and save the link/file to your local file system.\nUnzip the sample files. The tutorial assumes you unpacked files into one of the following directories:\nLinux/macOS:\n/tmp\nWindows:\nC:\\\\temp\nEach file has five data records. The data uses a comma (,) character as field\ndelimiter. The following is an example record:\nAlthea,Featherstone,afeatherstona@sf_tuts.com,\"8172 Browning Street, Apt B\",Calatrava,7/12/2017\nCopy\nThere are no blank spaces before or after the commas separating the\nfields in each record. This is the default that Snowflake expects when loading CSV data.\nLog in to SnowSQL\n\u00b6\nAfter you have\nSnowSQL\n, start SnowSQL to connect to Snowflake:\nOpen a command line window.\nStart SnowSQL:\n$\nsnowsql\n-a\n<account_identifier>\n-u\n<user_name>\nCopy\nWhere:\n<account_identifier>\nis the unique identifier for your Snowflake account.\nThe preferred format of the\naccount identifier\nis as follows:\norganization_name\n-\naccount_name\nNames of your Snowflake organization and account. For more information, see\nFormat 1 (preferred): Account name in your organization\n.\n<user_name>\nis the login name for your Snowflake user.\nNote\nIf your account has an identity provider (IdP) that has been defined for your account, you can use a web browser to authenticate instead of a password, as the following example demonstrates:\n$\nsnowsql\n-a\n<account_identifier>\n-u\n<user_name>\n--authenticator\nexternalbrowser\nCopy\nFor more information, see\nUsing a web browser for federated authentication/SSO\n.\nWhen SnowSQL prompts you, enter the password for your Snowflake user.\nIf you log in successfully, SnowSQL displays a command prompt that includes\nyour current warehouse, database, and schema.\nNote\nIf you get locked out of the account and can\u2019t obtain the account identifier, you can find it in the Welcome email that Snowflake sent to\nyou when you signed up for the trial account, or you can work with your\nORGADMIN to\nget the account details\n.\nYou can also find the values for\nlocator\n,\ncloud\n, and\nregion\nin the Welcome email.\nIf your Snowflake user doesn\u2019t have a default warehouse, database, and schema, or if\nyou didn\u2019t configure SnowSQL to specify a default warehouse, database, and schema,\nthe prompt displays\nno\nwarehouse\n,\nno\ndatabase\n, and\nno\nschema\n. For example:\nuser-name#(no warehouse)@(no database).(no schema)>\nCopy\nThis prompt indicates that there is no warehouse, database, and schema\nselected for the current session. You create these objects\nin the next step. As you follow the next steps in this tutorial to create\nthese objects, the prompt automatically updates to include the names of these objects.\nFor more information, see\nConnecting through SnowSQL\n.\nCreate Snowflake objects\n\u00b6\nDuring this step you create the following Snowflake objects:\nA database (\nsf_tuts\n) and a table (\nemp_basic\n). You load sample data into this table.\nA\nvirtual warehouse\n(\nsf_tuts_wh\n).\nThis warehouse provides the compute resources needed to load data into\nthe table and query the table. For this tutorial, you create an X-Small warehouse.\nAt the completion of this tutorial, you will remove these objects.\nCreate a database\n\u00b6\nCreate the\nsf_tuts\ndatabase using the\nCREATE DATABASE\ncommand:\nCREATE\nOR\nREPLACE\nDATABASE\nsf_tuts\n;\nCopy\nIn this tutorial, you use the default schema (\npublic\n) available for each database, rather than creating a new schema.\nNote that the database and schema you just created are now in use for your current\nsession, as reflected in the SnowSQL command prompt. You can also use the context\nfunctions to get this information.\nSELECT\nCURRENT_DATABASE\n(),\nCURRENT_SCHEMA\n();\nCopy\nThe following is an example result:\n+--------------------+------------------+\n| CURRENT_DATABASE() | CURRENT_SCHEMA() |\n|--------------------+------------------|\n| SF_TUTS\n| PUBLIC\n|\n+--------------------+------------------+\nCreate a table\n\u00b6\nCreate a table named\nemp_basic\nin\nsf_tuts.public\nusing the\nCREATE TABLE\ncommand:\nCREATE\nOR\nREPLACE\nTABLE\nemp_basic\n(\nfirst_name\nSTRING\n,\nlast_name\nSTRING\n,\nemail\nSTRING\n,\nstreetaddress\nSTRING\n,\ncity\nSTRING\n,\nstart_date\nDATE\n);\nCopy\nNote that the number of columns in the table, their positions, and their data types correspond to the fields in the sample CSV data files that you stage in the next step in this tutorial.\nCreate a virtual warehouse\n\u00b6\nCreate an X-Small warehouse named\nsf_tuts_wh\nusing the\nCREATE WAREHOUSE\ncommand:\nCREATE\nOR\nREPLACE\nWAREHOUSE\nsf_tuts_wh\nWITH\nWAREHOUSE_SIZE\n=\n'X-SMALL'\nAUTO_SUSPEND\n=\n180\nAUTO_RESUME\n=\nTRUE\nINITIALLY_SUSPENDED\n=\nTRUE\n;\nCopy\nThe\nsf_tuts_wh\nwarehouse is initially suspended, but the DML statement also sets\nAUTO_RESUME\n=\ntrue\n. The AUTO_RESUME setting causes a warehouse to automatically start\nwhen SQL statements that require compute resources are executed.\nAfter you create the warehouse, it\u2019s now in use for your current session.\nThis information is displayed in your SnowSQL command prompt. You can also retrieve\nthe name of the warehouse by using the following context function:\nSELECT\nCURRENT_WAREHOUSE\n();\nCopy\nThe following is an example result:\n+---------------------+\n| CURRENT_WAREHOUSE() |\n|---------------------|\n| SF_TUTS_WH\n|\n+---------------------+\nStage data files\n\u00b6\nA Snowflake stage is a location in cloud storage that you use to load and\nunload data from a table. Snowflake supports the following types of stages:\nInternal stages\n\u2014Used to store data files internally within Snowflake. Each user and table in Snowflake gets an internal stage by default for staging data files.\nExternal stages\n\u2014Used to store data files externally in Amazon S3, Google Cloud Storage, or Microsoft Azure.\nIf your data is already stored in these cloud storage services, you can use an external stage to load data in Snowflake tables.\nIn this tutorial, we upload the sample data files\n(downloaded in\nPrerequisites\n)\nto the internal stage for the\nemp_basic\ntable that you created earlier. You use the\nPUT\ncommand\nto upload the sample data files to that stage.\nStaging sample data files\n\u00b6\nExecute the\nPUT\ncommand in\nSnowSQL\nto upload local data files to the table stage\nprovided for the\nemp_basic\ntable you created.\nPUT\nfile\n://<\nfile\n-\npath\n>[/\\]\nemployees0\n*.\ncsv\n@\nsf_tuts\n.\npublic\n.%\nemp_basic\n;\nCopy\nFor example:\nLinux or macOS\nPUT\nfile\n:///\ntmp\n/\nemployees0\n*.\ncsv\n@\nsf_tuts\n.\npublic\n.%\nemp_basic\n;\nCopy\nWindows\nPUT\nfile\n://\nC\n:\\\ntemp\n\\\nemployees0\n*.\ncsv\n@\nsf_tuts\n.\npublic\n.%\nemp_basic\n;\nCopy\nLet\u2019s take a closer look at the command:\nfile://<file-path>[/]employees0*.csv\nspecifies the full directory path and\nnames of the files on your local machine to stage. Note that file system wildcards are allowed, and if multiple files fit the pattern they are all displayed.\n@<namespace>.%<table_name>\nindicates to use the stage for the specified table, in this case the\nemp_basic\ntable.\nThe command returns the following result, showing the staged files:\n+-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------+\n| source\n| target\n| source_size | target_size | source_compression | target_compression | status\n| message |\n|-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------|\n| employees01.csv | employees01.csv.gz |\n360 |\n287 | NONE\n| GZIP\n| UPLOADED |\n|\n| employees02.csv | employees02.csv.gz |\n355 |\n274 | NONE\n| GZIP\n| UPLOADED |\n|\n| employees03.csv | employees03.csv.gz |\n397 |\n295 | NONE\n| GZIP\n| UPLOADED |\n|\n| employees04.csv | employees04.csv.gz |\n366 |\n288 | NONE\n| GZIP\n| UPLOADED |\n|\n| employees05.csv | employees05.csv.gz |\n394 |\n299 | NONE\n| GZIP\n| UPLOADED |\n|\n+-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------+\nThe PUT command compresses files by default using\ngzip\n, as indicated in the TARGET_COMPRESSION column.\nListing the staged files (Optional)\n\u00b6\nYou can list the staged files using the\nLIST\ncommand.\nLIST\n@\nsf_tuts\n.\npublic\n.%\nemp_basic\n;\nCopy\nThe following is an example result:\n+--------------------+------+----------------------------------+------------------------------+\n| name\n| size | md5\n| last_modified\n|\n|--------------------+------+----------------------------------+------------------------------|\n| employees01.csv.gz |\n288 | a851f2cc56138b0cd16cb603a97e74b1 | Tue, 9 Jan 2018 15:31:44 GMT |\n| employees02.csv.gz |\n288 | 125f5645ea500b0fde0cdd5f54029db9 | Tue, 9 Jan 2018 15:31:44 GMT |\n| employees03.csv.gz |\n304 | eafee33d3e62f079a054260503ddb921 | Tue, 9 Jan 2018 15:31:45 GMT |\n| employees04.csv.gz |\n304 | 9984ab077684fbcec93ae37479fa2f4d | Tue, 9 Jan 2018 15:31:44 GMT |\n| employees05.csv.gz |\n304 | 8ad4dc63a095332e158786cb6e8532d0 | Tue, 9 Jan 2018 15:31:44 GMT |\n+--------------------+------+----------------------------------+------------------------------+\nCopy data into target tables\n\u00b6\nTo load your staged data into the target table, execute\nCOPY INTO <table>\n.\nThe\nCOPY INTO <table>\ncommand uses the virtual warehouse you created\nin\nCreate Snowflake objects\nto copy files.\nCOPY\nINTO\nemp_basic\nFROM\n@%\nemp_basic\nFILE_FORMAT\n=\n(\ntype\n=\ncsv\nfield_optionally_enclosed_by\n=\n'\"'\n)\nPATTERN\n=\n'.*employees0[1-5].csv.gz'\nON_ERROR\n=\n'skip_file'\n;\nCopy\nWhere:\nThe FROM clause specifies the location containing the data files (the internal stage for the table).\nThe FILE_FORMAT clause specifies the file type as CSV, and specifies the double-quote\ncharacter (\n\"\n) as the character used to enclose strings. Snowflake supports\ndiverse file types and options. These are described\nin\nCREATE FILE FORMAT\n.\nThe PATTERN clause specifies that the command should load data from the filenames matching\nthis regular expression (\n.*employees0[1-5].csv.gz\n).\nThe ON_ERROR clause specifies what to do when the COPY command encounters errors in the files. By default, the command stops loading data\nwhen the first error is encountered. This example skips any file containing an error and moves on to loading\nthe next file. (None of the files in this tutorial contain errors; this is included for illustration purposes.)\nThe COPY command also provides an option for validating files before they are loaded. For more information about additional error checking and validation instructions, see the\nCOPY INTO <table>\ntopic and the other\ndata loading tutorials\n.\nThe COPY command returns a result showing the list of files copied and related information:\n+--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+\n| file\n| status | rows_parsed | rows_loaded | error_limit | errors_seen | first_error | first_error_line | first_error_character | first_error_column_name |\n|--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------|\n| employees02.csv.gz | LOADED |\n5 |\n5 |\n1 |\n0 | NULL\n|\nNULL |\nNULL | NULL\n|\n| employees04.csv.gz | LOADED |\n5 |\n5 |\n1 |\n0 | NULL\n|\nNULL |\nNULL | NULL\n|\n| employees05.csv.gz | LOADED |\n5 |\n5 |\n1 |\n0 | NULL\n|\nNULL |\nNULL | NULL\n|\n| employees03.csv.gz | LOADED |\n5 |\n5 |\n1 |\n0 | NULL\n|\nNULL |\nNULL | NULL\n|\n| employees01.csv.gz | LOADED |\n5 |\n5 |\n1 |\n0 | NULL\n|\nNULL |\nNULL | NULL\n|\n+--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+\nQuery loaded data\n\u00b6\nYou can query the data loaded in the\nemp_basic\ntable using standard\nSQL\nand any supported\nfunctions\nand\noperators\n.\nYou can also manipulate the data, such as updating the loaded data or inserting more data, using standard\nDML commands\n.\nRetrieve all data\n\u00b6\nReturn all rows and columns from the table:\nSELECT\n*\nFROM\nemp_basic\n;\nCopy\nThe following is a partial result:\n+\n------------+--------------+---------------------------+-----------------------------+--------------------+------------+\n| FIRST_NAME | LAST_NAME\n| EMAIL\n| STREETADDRESS\n| CITY\n| START_DATE |\n|------------+--------------+---------------------------+-----------------------------+--------------------+------------|\n| Arlene\n| Davidovits\n| adavidovitsk@sf_tuts.com\n| 7571 New Castle Circle\n| Meniko\n| 2017-05-03 |\n| Violette\n| Shermore\n| vshermorel@sf_tuts.com\n| 899 Merchant Center\n| Troitsk\n| 2017-01-19 |\n| Ron\n| Mattys\n| rmattysm@sf_tuts.com\n| 423 Lien Pass\n| Bayaguana\n| 2017-11-15 |\n...\n...\n...\n| Carson\n| Bedder\n| cbedderh@sf_tuts.co.au\n| 71 Clyde Gallagher Place\n| Leninskoye\n| 2017-03-29 |\n| Dana\n| Avory\n| davoryi@sf_tuts.com\n| 2 Holy Cross Pass\n| Wenlin\n| 2017-05-11 |\n| Ronny\n| Talmadge\n| rtalmadgej@sf_tuts.co.uk\n| 588 Chinook Street\n| Yawata\n| 2017-06-02 |\n+\n------------+--------------+---------------------------+-----------------------------+--------------------+------------+\nCopy\nInsert additional data rows\n\u00b6\nIn addition to loading data from staged files into a table, you can insert rows directly into a table using the\nINSERT\nDML command.\nFor example, to insert two additional rows into the table:\nINSERT\nINTO\nemp_basic\nVALUES\n(\n'Clementine'\n,\n'Adamou'\n,\n'cadamou@sf_tuts.com'\n,\n'10510 Sachs Road'\n,\n'Klenak'\n,\n'2017-9-22'\n)\n,\n(\n'Marlowe'\n,\n'De Anesy'\n,\n'madamouc@sf_tuts.co.uk'\n,\n'36768 Northfield Plaza'\n,\n'Fangshan'\n,\n'2017-1-26'\n);\nCopy\nQuery rows based on email address\n\u00b6\nReturn a list of email addresses with United Kingdom top-level domains using the\n[ NOT ] LIKE\nfunction:\nSELECT\nemail\nFROM\nemp_basic\nWHERE\nemail\nLIKE\n'%.uk'\n;\nCopy\nThe following is an example result:\n+--------------------------+\n| EMAIL\n|\n|--------------------------|\n| gbassfordo@sf_tuts.co.uk |\n| rtalmadgej@sf_tuts.co.uk |\n| madamouc@sf_tuts.co.uk\n|\n+--------------------------+\nQuery rows based on start date\n\u00b6\nFor example, to calculate when certain employee benefits might start, add 90 days to employee start\ndates using the\nDATEADD\nfunction. Filter the list by employees whose start date occurred earlier than January 1, 2017:\nSELECT\nfirst_name\n,\nlast_name\n,\nDATEADD\n(\n'day'\n,\n90\n,\nstart_date\n)\nFROM\nemp_basic\nWHERE\nstart_date\n<=\n'2017-01-01'\n;\nCopy\nThe following is an example result:\n+------------+-----------+------------------------------+\n| FIRST_NAME | LAST_NAME | DATEADD('DAY',90,START_DATE) |\n|------------+-----------+------------------------------|\n| Granger\n| Bassford\n| 2017-03-30\n|\n| Catherin\n| Devereu\n| 2017-03-17\n|\n| Cesar\n| Hovie\n| 2017-03-21\n|\n| Wallis\n| Sizey\n| 2017-03-30\n|\n+------------+-----------+------------------------------+\nSummary, clean up, and additional resources\n\u00b6\nCongratulations! You\u2019ve successfully completed this introductory tutorial.\nTake a few minutes to review a short summary and the key points covered in the tutorial.\nYou might also want to consider cleaning up by dropping any objects you created in the tutorial.\nLearn more by reviewing other topics in the Snowflake Documentation.\nSummary and key points\n\u00b6\nIn summary, data loading is performed in two steps:\nStage the data files to load. The files can be staged internally (in Snowflake) or in an external location. In this tutorial, you stage files internally.\nCopy data from the staged files into an existing target table. A running\nwarehouse is required for this step.\nRemember the following key points about loading CSV files:\nA CSV file consists of 1 or more records, with 1 or more fields in each record, and sometimes a header record.\nRecords and fields in each file are separated by delimiters. The default delimiters are:\nRecords\n:\nnewline characters\nFields\n:\ncommas\nIn other words, Snowflake expects each record in a CSV file to be separated by new lines and the fields (i.e. individual values) in each record to be separated by commas. If\ndifferent\ncharacters are used as record and field delimiters, you must\nexplicitly\nspecify this as part of the file format when loading.\nThere is a\ndirect\ncorrelation between the fields in the files and the columns in the table you will be loading, in terms of:\nNumber of fields (in the file) and columns (in the target table).\nPositions of the fields and columns within their respective file/table.\nData types, such as string, number, or date, for fields and columns.\nThe records will not be loaded if the numbers, positions, and data types don\u2019t align with the data.\nNote\nSnowflake supports loading files in which the fields don\u2019t exactly align with the columns in the target table;\nhowever, this is a more advanced data loading topic (covered in\nTransforming data during a load\n).\nTutorial cleanup (Optional)\n\u00b6\nIf the objects you created in this tutorial are no longer needed,\nyou can remove them from the system with\nDROP <object>\nstatements.\nDROP\nDATABASE\nIF\nEXISTS\nsf_tuts\n;\nDROP\nWAREHOUSE\nIF\nEXISTS\nsf_tuts_wh\n;\nCopy\nExit the connection\n\u00b6\nTo exit a connection, use the\n!exit\ncommand for SnowSQL (or its alias,\n!disconnect\n).\nExit drops the current connection and quits SnowSQL if it is the last connection.\nWhat\u2019s next?\n\u00b6\nContinue learning about Snowflake using the following resources:\nComplete the other tutorials provided by Snowflake:\nSnowflake Tutorials\nFamiliarize yourself with key Snowflake concepts and features, as well as the SQL commands to perform queries and insert/update data:\nIntroduction to Snowflake\nQuery syntax\nData Manipulation Language (DML) commands\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/user-guide/intro-key-concepts": {"title": "Key Concepts & Architecture | Snowflake Documentation", "content": "Key Concepts & Architecture | Snowflake Documentation\nKey Concepts & Architecture\n\u00b6\nSnowflake\u2019s Data Cloud is powered by an advanced data platform provided as a self-managed service. Snowflake enables data storage,\nprocessing, and analytic solutions that are faster, easier to use, and far more flexible than traditional offerings.\nThe Snowflake data platform is not built on any existing database technology or \u201cbig data\u201d software platforms such as Hadoop. Instead, Snowflake combines a completely new SQL query engine with an innovative architecture natively designed for the cloud. To the user,\nSnowflake provides all of the functionality of an enterprise analytic database, along with many additional special features and unique capabilities.\nData Platform as a Self-managed Service\n\u00b6\nSnowflake is a true self-managed service, meaning:\nThere is no hardware (virtual or physical) to select, install, configure, or manage.\nThere is virtually no software to install, configure, or manage.\nOngoing maintenance, management, upgrades, and tuning are handled by Snowflake.\nSnowflake runs completely on cloud infrastructure. All components of Snowflake\u2019s service (other than optional command line clients, drivers, and connectors), run in public cloud infrastructures.\nSnowflake uses virtual compute instances for its compute needs and a storage service for persistent storage of data. Snowflake cannot be run on private cloud infrastructures (on-premises or\nhosted).\nSnowflake is not a packaged software offering that can be installed by a user. Snowflake manages all aspects of software installation and updates.\nSnowflake Architecture\n\u00b6\nSnowflake\u2019s architecture is a hybrid of traditional shared-disk and shared-nothing database architectures. Similar to shared-disk architectures, Snowflake uses a central data repository for persisted data that is\naccessible from all compute nodes in the platform. But similar to shared-nothing architectures, Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of\nthe entire data set locally. This approach offers the data management simplicity of a shared-disk architecture, but with the performance and scale-out benefits of a shared-nothing architecture.\nSnowflake\u2019s unique architecture consists of three key layers:\nDatabase Storage\nQuery Processing\nCloud Services\nDatabase Storage\n\u00b6\nWhen data is loaded into Snowflake, Snowflake reorganizes that data into its internal optimized, compressed, columnar format. Snowflake stores this optimized data in cloud storage.\nSnowflake manages all aspects of how this data is stored \u2014 the organization, file size, structure, compression, metadata, statistics, and other aspects of data storage are handled by Snowflake. The data objects stored by Snowflake are not directly visible nor accessible by customers; they are only accessible through SQL query operations run using Snowflake.\nQuery Processing\n\u00b6\nQuery execution is performed in the processing layer. Snowflake processes queries using \u201cvirtual warehouses\u201d. Each virtual warehouse is an MPP compute cluster composed of multiple compute nodes allocated by Snowflake from a cloud provider.\nEach virtual warehouse is an independent compute cluster that does not share compute resources with other virtual warehouses. As a result, each virtual warehouse has no impact on the performance of other virtual warehouses.\nFor more information, see\nVirtual warehouses\n.\nCloud Services\n\u00b6\nThe cloud services layer is a collection of services that coordinate activities across Snowflake. These services tie together all of the different components of Snowflake in order to process user requests, from login to query dispatch.\nThe cloud services layer also runs on compute instances provisioned by Snowflake from the cloud provider.\nServices managed in this layer include:\nAuthentication\nInfrastructure management\nMetadata management\nQuery parsing and optimization\nAccess control\nConnecting to Snowflake\n\u00b6\nSnowflake supports multiple ways of connecting to the service:\nA web-based user interface from which all aspects of managing and using Snowflake can be accessed.\nCommand line clients (e.g. SnowSQL) which can also access all aspects of managing and using Snowflake.\nODBC and JDBC drivers that can be used by other applications (e.g. Tableau) to connect to Snowflake.\nNative connectors (e.g. Python, Spark) that can be used to develop applications for connecting to Snowflake.\nThird-party connectors that can be used to connect applications such as ETL tools (e.g. Informatica) and BI tools (e.g. ThoughtSpot) to Snowflake.\nFor more information, see\nLogging in to Snowflake\n.\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/user-guide-getting-started": {"title": "Getting Started - Snowflake Documentation", "content": "Getting Started - Snowflake Documentation\nGetting Started\nBegin your adventure: a comprehensive guide to getting started with Snowflake and making the most of its features and benefits.\nIntroduction to Snowflake\nBasic information and instructions for first-time users of Snowflake.\nBefore you begin\nMake sure that you meet the prerequisites for using our applications that connect to Snowflake.\nLogging into Snowflake\nLearn how to use Snowsight or SnowSQL to log into Snowflake.\nSnowsight Quick Tour\nLearn about the features in Snowsight that you can use to manage database objects and query data.\nLearn the Key Concepts\nThese topics introduce the Snowflake architecture and basic features\nArchitecture\nLearn about Snowflake\u2019s architecture, how data is stored, and how queries are processed.\nCloud Platforms\nLearn about the cloud platforms that are supported for use with Snowflake.\nCloud Regions\nLearn about the cloud provider regions in which Snowflake accounts are hosted.\nEditions\nLearn about the different editions provided by Snowflake and the features and levels of service offered with each edition.\nReleases\nLearn about our process for deploying new releases.\nFeatures\nLearn about some of the key features of Snowflake.\nManage Your Data\nData Lifecycle\nLearn about how data is organized, stored, queried, and managed.\nContinuous Data Protection\nLearn about how your data is protected and how you can access and recover your data.\nRegulatory Compliance\nLearn about our commitment to meeting industry-standard regulatory compliance requirements.\nStart Learning\nTUTORIAL\nSnowflake in 20 Minutes\nLearn how to create Snowflake objects, load data, and query data.\nTUTORIAL\nCreate Users and Grant Roles\nLearn how to create a user and grant roles to it using SQL.\nTUTORIAL\nLoad and Query Sample Data Using SQL\nLearn how to load and query Tasty Bytes sample data using SQL.\nTUTORIAL\nLoad and Query Sample Data Using Snowpark Python\nLearn how to load and query Tasty Bytes sample data using Python.\nTUTORIAL\nLoad Data from Cloud Storage (Amazon S3)\nLearn how to load a table from an S3 bucket.\nTUTORIAL\nLoad Data from Cloud Storage (Microsoft Azure)\nLearn how to load a table from an Azure container.\nTUTORIAL\nLoad Data from Cloud Storage (Google)\nLearn how to load a table from a GCS bucket.\nTUTORIAL\nBulk Loading from a Local File System Using COPY\nLearn how to load data from files in your local file system into a table.\nTUTORIAL\nBulk Loading from Amazon S3 Using COPY\nLearn how to load data from files in an existing Amazon Simple Storage Service (Amazon S3) bucket into a table.\nTUTORIAL\nJSON Basics\nLearn how to load JSON data from files, query the data, and flatten and store the data in a table.\nTUTORIAL\nLoading JSON Data into a Relational Table\nLearn more about how to store JSON objects, extract JSON elements, and transform those elements into table columns.\nTUTORIAL\nLoading and Unloading Parquet Data\nLearn how to upload Parquet data by transforming elements of a staged Parquet file directly into table columns.\nSample Data Sets\nSnowflake provides sample data sets, such as the industry-standard TPC-DS and TPC-H benchmarks, for evaluating and testing a broad range of Snowflake\u2019s SQL support.\nSee all\nSNOWFLAKE_SAMPLE_DATA\nThe sample database, SNOWFLAKE_SAMPLE_DATA, is identical to the databases that you create in your account, except that it is read-only.\nTPC-DS\nTPC-DS models the decision support functions of a retail product supplier. The supporting schema contains vital business information, such as customer, order, and product data.\nTPC-H\nTPC-H is a decision support benchmark. It consists of a suite of business-oriented ad hoc queries and concurrent data modifications.\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}, "https://docs.snowflake.com/en/user-guide/data-time-travel": {"title": "Understanding & using Time Travel | Snowflake Documentation", "content": "Understanding & using Time Travel | Snowflake Documentation\nUnderstanding & using Time Travel\n\u00b6\nSnowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period.\nIt serves as a powerful tool for performing the following tasks:\nRestoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.\nDuplicating and backing up data from key points in the past.\nAnalyzing data usage/manipulation over specified periods of time.\nIntroduction to Time Travel\n\u00b6\nUsing Time Travel, you can perform the following actions within a defined period of time:\nQuery data in the past that has since been updated or deleted.\nCreate clones of entire tables, schemas, and databases at or before specific points in the past.\nRestore tables, schemas, and databases that have been dropped.\nNote\nWhen querying historical data in a table or non-materialized view, the current table or view schema is used. For more\ninformation, see\nUsage notes\nfor AT | BEFORE.\nAfter the defined period of time has elapsed, the data is moved into\nSnowflake Fail-safe\nand these actions\ncan no longer be performed.\nNote\nA long-running Time Travel query will delay moving any data and objects (tables, schemas, and databases) in the account into Fail-safe,\nuntil the query completes.\nTime Travel SQL extensions\n\u00b6\nTo support Time Travel, the following SQL extensions have been implemented:\nAT | BEFORE\nclause which can be specified in SELECT statements and CREATE \u2026 CLONE commands (immediately\nafter the object name). The clause uses one of the following parameters to pinpoint the exact historical data you want to access:\nTIMESTAMP\nOFFSET (time difference in seconds from the present time)\nSTATEMENT (query ID for statement)\nUNDROP command for tables, schemas, and databases.\nData retention period\n\u00b6\nA key component of Snowflake Time Travel is the data retention period.\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\nTime Travel operations (SELECT, CREATE \u2026 CLONE, UNDROP) can be performed on the data.\nThe standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts:\nFor Snowflake Standard Edition, the retention period can be set to 0 (or unset back to the default of 1 day) at the account and object\nlevel (i.e. databases, schemas, and tables).\nFor Snowflake Enterprise Edition (and higher):\nFor transient databases, schemas, and tables, the retention period can be set to 0 (or unset back to the default of 1 day). The same\nis also true for temporary tables.\nFor permanent databases, schemas, and tables, the retention period can be set to any value from 0 up to 90 days.\nNote\nA retention period of 0 days for an object effectively deactivates Time Travel for the object.\nWhen the retention period ends for an object, the historical data is moved into\nSnowflake Fail-safe\n:\nHistorical data is no longer available for querying.\nPast objects can no longer be cloned.\nPast objects that were dropped can no longer be restored.\nTo specify the data retention period for Time Travel:\nThe\nDATA_RETENTION_TIME_IN_DAYS\nobject parameter can be used by users with the ACCOUNTADMIN role to set the default\nretention period for your account.\nThe same parameter can be used to explicitly override the default when creating a database, schema, and individual table.\nThe data retention period for a database, schema, or table can be changed at any time.\nThe\nMIN_DATA_RETENTION_TIME_IN_DAYS\naccount parameter can be set by users with the ACCOUNTADMIN role to set a minimum\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nEnabling and deactivating Time Travel\n\u00b6\nNo tasks are required to enable Time Travel. It is automatically enabled with the standard, 1-day retention period.\nHowever, you may want to upgrade to Snowflake Enterprise Edition to enable configuring longer data retention periods of up to 90 days\nfor databases, schemas, and tables. Note that extended data retention requires additional storage which will be reflected in your monthly\nstorage charges. For more information about storage charges, see\nStorage Costs for Time Travel and Fail-safe\n.\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set\nDATA_RETENTION_TIME_IN_DAYS\nto 0 at\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\nby default; however, this default can be overridden at any time for any database, schema, or table.\nA user with the ACCOUNTADMIN role can also set the\nMIN_DATA_RETENTION_TIME_IN_DAYS\nat the account level. This parameter\nsetting enforces a minimum data retention period for databases, schemas, and tables. Setting MIN_DATA_RETENTION_TIME_IN_DAYS does not\nalter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. It may, however, change the effective data retention period for objects.\nWhen MIN_DATA_RETENTION_TIME_IN_DAYS is set at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nTime Travel can be deactivated for individual databases, schemas, and tables by specifying\nDATA_RETENTION_TIME_IN_DAYS\nwith a\nvalue of 0 for the object. However, if DATA_RETENTION_TIME_IN_DAYS is set to a value of 0, and MIN_DATA_RETENTION_TIME_IN_DAYS is set\nat the account level and is greater than 0, the higher value setting takes precedence.\nAttention\nBefore setting\nDATA_RETENTION_TIME_IN_DAYS\nto 0 for any object, consider whether you want to deactivate Time Travel for the object,\nparticularly as it pertains to recovering the object if it is dropped. When an object with no retention period is dropped, you will not\nbe able to restore the object.\nAs a general rule, we recommend maintaining a value of (at least) 1 day for any given object.\nIf the Time Travel retention period is set to 0, any modified or deleted data is moved into Fail-safe (for permanent tables)\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\nSpecifying the data retention period for an object\n\u00b6\nBy default, the maximum retention period is 1 day (i.e. one 24 hour period). With Snowflake Enterprise Edition (and higher), the default\nfor your account can be set to any value up to 90 days:\nWhen creating a table, schema, or database, the account default can be overridden using the\nDATA_RETENTION_TIME_IN_DAYS\nparameter in the command.\nIf a retention period is specified for a database or schema, the period is inherited by default for all objects created in the\ndatabase/schema.\nA minimum retention period can be set on the account using the\nMIN_DATA_RETENTION_TIME_IN_DAYS\nparameter. If this parameter is\nset at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nChanging the data retention period for an object\n\u00b6\nIf you change the data retention period for a table, the new retention period impacts all data that is active, as well as any data currently\nin Time Travel. The impact depends on whether you increase or decrease the period:\nIncreasing Retention\n:\nCauses the data currently in Time Travel to be retained for the longer time period.\nFor example, if you have a table with a 10-day retention period and increase the period to 20 days, data that would have been removed\nafter 10 days is now retained for an additional 10 days before moving into Fail-safe.\nNote that this doesn\u2019t apply to any data that is older than 10 days and has already moved into Fail-safe.\nDecreasing Retention\n:\nReduces the amount of time data is retained in Time Travel:\nFor active data modified after the retention period is reduced, the new shorter period applies.\nFor data that is currently in Time Travel:\nIf the data is still within the new shorter period, it remains in Time Travel.\nIf the data is outside the new period, it moves into Fail-safe.\nFor example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved\ninto Fail-safe, leaving only the data from day 1 accessible through Time Travel.\nHowever, the process of moving the data from Time Travel into Fail-safe is performed by a background process, so the change is not immediately\nvisible. Snowflake guarantees that the data will be moved, but does not specify when the process will complete; until the background process\ncompletes, the data is still accessible through Time Travel.\nNote\nIf you change the data retention period for a database or schema, the change only affects active objects contained within\nthe database or schema. Any objects that have been dropped (for example, tables) remain unaffected.\nFor example, if you have a schema\ns1\nwith a 90-day retention period and table\nt1\nis in schema\ns1\n,\ntable\nt1\ninherits the 90-day retention period. If you drop table\ns1.t1\n,\nt1\nis retained in Time Travel\nfor 90 days. Later, if you change the schema\u2019s data retention period to 1 day, the retention\nperiod for the dropped table\nt1\nis unchanged. Table\nt1\nwill still be retained in Time Travel for 90 days.\nTo alter the retention period of a dropped object, you must undrop the object, then alter its retention period.\nTo change the retention period for an object, use the appropriate\nALTER <object>\ncommand. For example, to change the\nretention period for a table:\nCREATE\nTABLE\nmytable\n(\ncol1\nNUMBER\n,\ncol2\nDATE\n)\nDATA_RETENTION_TIME_IN_DAYS\n=\n90\n;\nALTER\nTABLE\nmytable\nSET\nDATA_RETENTION_TIME_IN_DAYS\n=\n30\n;\nCopy\nAttention\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\nretention period explicitly set. For example:\nIf you change the retention period at the account level, all databases, schemas, and tables that do not have an explicit retention period\nautomatically inherit the new retention period.\nIf you change the retention period at the schema level, all tables in the schema that do not have an explicit retention period inherit the\nnew retention period.\nKeep this in mind when changing the retention period for your account or any objects in your account because the change might have\nTime Travel consequences that you did not anticipate or intend. In particular, we do\nnot\nrecommend changing the retention period to 0\nat the account level.\nDropped containers and object retention inheritance\n\u00b6\nCurrently, when a database is dropped, the data retention period for child schemas or tables, if explicitly set to be different from the\nretention of the database, is not honored. The child schemas or tables are retained for the same period of time as the database.\nSimilarly, when a schema is dropped, the data retention period for child tables, if explicitly set to be different from the retention of\nthe schema, is not honored. The child tables are retained for the same period of time as the schema.\nTo honor the data retention period for these child objects (schemas or tables), drop them explicitly\nbefore\nyou drop the database\nor schema.\nQuerying historical data\n\u00b6\nWhen any DML operations are performed on a table, Snowflake retains previous versions of the table data for a defined period of time. This\nenables querying earlier versions of the data using the\nAT | BEFORE\nclause.\nThis clause supports querying data either exactly at or immediately preceding a specified point in the table\u2019s history within the\nretention period. The specified point can be time-based (e.g. a timestamp or time offset from the present) or it can be the ID for a\ncompleted statement (e.g. SELECT or INSERT).\nFor example:\nThe following query selects historical data from a table as of the date and time represented by the specified\ntimestamp\n:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 09:20:00 -0700'\n::timestamp_tz\n);\nCopy\nThe following query selects historical data from a table as of 5 minutes ago:\nSELECT\n*\nFROM\nmy_table\nAT\n(\nOFFSET\n=>\n-\n60\n*\n5\n);\nCopy\nThe following query selects historical data from a table up to, but not including any changes made by the specified statement:\nSELECT\n*\nFROM\nmy_table\nBEFORE\n(\nSTATEMENT\n=>\n'8e5d0ca9-005e-44e6-b858-a8f5b37c5726'\n);\nCopy\nNote\nIf the TIMESTAMP, OFFSET, or STATEMENT specified in the\nAT | BEFORE\nclause falls outside the data\nretention period for the table, the query fails and returns an error.\nCloning historical objects\n\u00b6\nIn addition to queries, the\nAT | BEFORE\nclause can be used with the CLONE keyword in the CREATE command\nfor a table, schema, or database to create a logical duplicate of the object at a specified point in the object\u2019s history. If you don\u2019t\nspecify a point in time, the clone defaults to the state of the object as of now\n(the\nCURRENT_TIMESTAMP\nvalue).\nFor example:\nThe following\nCREATE TABLE\nstatement creates a clone of a table as of the date and time represented by the\nspecified timestamp:\nCREATE\nTABLE\nrestored_table\nCLONE\nmy_table\nAT\n(\nTIMESTAMP\n=>\n'Wed, 26 Jun 2024 01:01:00 +0300'\n::timestamp_tz\n);\nCopy\nThe following\nCREATE SCHEMA\nstatement creates a clone of a schema and all its objects as they existed 1 hour\nbefore the current time:\nCREATE\nSCHEMA\nrestored_schema\nCLONE\nmy_schema\nAT\n(\nOFFSET\n=>\n-\n3600\n);\nCopy\nThe following\nCREATE DATABASE\nstatement creates a clone of a database and all its objects as they existed prior\nto the completion of the specified statement:\nCREATE\nDATABASE\nrestored_db\nCLONE\nmy_db\nBEFORE\n(\nSTATEMENT\n=>\n'8e5d0ca9-005e-44e6-b858-a8f5b37c5726'\n);\nCopy\nNote\nThe cloning operation for a database or schema fails:\nIf the specified Time Travel time is beyond the retention time of\nany current child\n(e.g., a table) of the entity.\nAs a workaround for child objects that have been purged from Time Travel, use the\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION\nparameter of the\nCREATE <object> \u2026 CLONE command. For more information, see\nChild objects and data retention time\n.\nIf the specified Time Travel time is at or before the point in time when the object was created.\nThe following\nCREATE DATABASE\nstatement creates a clone of a database and all its objects as they existed\nfour days ago, skipping any tables that have a data retention period of less than four days:\nCREATE\nDATABASE\nrestored_db\nCLONE\nmy_db\nAT\n(\nTIMESTAMP\n=>\nDATEADD\n(\ndays\n,\n-\n4\n,\ncurrent_timestamp\n)\n::timestamp_tz\n)\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION\n;\nCopy\nDropping and restoring objects\n\u00b6\nDropping objects\n\u00b6\nWhen a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. Instead, it is retained for the\ndata retention period for the object, during which time the object can be restored. Once dropped objects are moved to\nFail-safe\n, you cannot restore them.\nTo drop a table, schema, or database, use the following commands:\nDROP TABLE\nDROP SCHEMA\nDROP DATABASE\nNote\nAfter dropping an object, creating an object with the same name does not restore the object. Instead, it creates a new version of the\nobject. The original, dropped version is still available and can be restored.\nRestoring a dropped object restores the object in place (i.e. it does not create a new object).\nListing dropped objects\n\u00b6\nDropped tables, schemas, and databases can be listed using the following commands with the HISTORY keyword specified:\nSHOW TABLES\nSHOW SCHEMAS\nSHOW DATABASES\nFor example:\nSHOW\nTABLES\nHISTORY\nLIKE\n'load%'\nIN\nmytestdb\n.\nmyschema\n;\nSHOW\nSCHEMAS\nHISTORY\nIN\nmytestdb\n;\nSHOW DATABASES\nHISTORY\n;\nCopy\nThe output includes all dropped objects and an additional DROPPED_ON column, which displays the date and time when the object was dropped.\nIf an object has been dropped more than once, each version of the object is included as a separate row in the output.\nNote\nAfter the retention period for an object has passed and the object has been purged, it is no longer displayed in the\nSHOW\n<object_type>\nHISTORY output.\nRestoring objects\n\u00b6\nA dropped object that has not been purged from the system (i.e. the object is displayed in the SHOW\n<object_type>\nHISTORY output) can be\nrestored using the following commands:\nUNDROP TABLE\nUNDROP SCHEMA\nUNDROP DATABASE\nCalling UNDROP restores the object to its most recent state before the DROP command was issued.\nFor example:\nUNDROP\nTABLE\nmytable\n;\nUNDROP\nSCHEMA\nmyschema\n;\nUNDROP\nDATABASE\nmydatabase\n;\nCopy\nNote\nIf an object with the same name already exists, UNDROP fails. You must rename the existing object, which then enables you to restore\nthe previous version of the object.\nAccess control requirements and name resolution\n\u00b6\nSimilar to dropping an object, a user must have OWNERSHIP privileges for an object to restore it. In addition, the user must have CREATE\nprivileges on the object type for the database or schema where the dropped object will be restored.\nRestoring tables and schemas is only supported in the current schema or current database, even if a fully-qualified object name is specified.\nExample: Dropping and restoring a table multiple times\n\u00b6\nIn the following example, the\nmytestdb.public\nschema contains two tables:\nloaddata1\nand\nproddata1\n. The\nloaddata1\ntable is\ndropped and recreated twice, creating three versions of the table:\nCurrent version\nSecond (i.e. most recent) dropped version\nFirst dropped version\nThe example then illustrates how to restore the two dropped versions of the table:\nFirst, the current table with the same name is renamed to\nloaddata3\n. This enables restoring the most recent version of the dropped\ntable, based on the timestamp.\nThen, the most recent dropped version of the table is restored.\nThe restored table is renamed to\nloaddata2\nto enable restoring the first version of the dropped table.\nLastly, the first version of the dropped table is restored.\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on\n| name\n| database_name | schema_name | kind\n| comment | cluster_by | rows | bytes | owner\n| retention_time | dropped_on\n|\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 48\n| 16248 | PUBLIC | 1\n| [NULL]\n|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 12\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nDROP\nTABLE\nloaddata1\n;\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on\n| name\n| database_name | schema_name | kind\n| comment | cluster_by | rows | bytes | owner\n| retention_time | dropped_on\n|\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 12\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 48\n| 16248 | PUBLIC | 1\n| Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nCREATE\nTABLE\nloaddata1\n(\nc1\nnumber\n);\nINSERT\nINTO\nloaddata1\nVALUES\n(\n1111\n),\n(\n2222\n),\n(\n3333\n),\n(\n4444\n);\nDROP\nTABLE\nloaddata1\n;\nCREATE\nTABLE\nloaddata1\n(\nc1\nvarchar\n);\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on\n| name\n| database_name | schema_name | kind\n| comment | cluster_by | rows | bytes | owner\n| retention_time | dropped_on\n|\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 0\n| 0\n| PUBLIC | 1\n| [NULL]\n|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 12\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 4\n| 4096\n| PUBLIC | 1\n| Fri, 13 May 2016 19:05:51 -0700 |\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 48\n| 16248 | PUBLIC | 1\n| Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nALTER\nTABLE\nloaddata1\nRENAME\nTO\nloaddata3\n;\nUNDROP\nTABLE\nloaddata1\n;\nSHOW\nTABLES\nHISTORY\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on\n| name\n| database_name | schema_name | kind\n| comment | cluster_by | rows | bytes | owner\n| retention_time | dropped_on\n|\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 4\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 0\n| 0\n| PUBLIC | 1\n| [NULL]\n|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 12\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 48\n| 16248 | PUBLIC | 1\n| Fri, 13 May 2016 19:04:46 -0700 |\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nALTER\nTABLE\nloaddata1\nRENAME\nTO\nloaddata2\n;\nUNDROP\nTABLE\nloaddata1\n;\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\n| created_on\n| name\n| database_name | schema_name | kind\n| comment | cluster_by | rows | bytes | owner\n| retention_time | dropped_on\n|\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 48\n| 16248 | PUBLIC | 1\n| [NULL]\n|\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA2 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 4\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 0\n| 0\n| PUBLIC | 1\n| [NULL]\n|\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB\n| PUBLIC\n| TABLE |\n|\n| 12\n| 4096\n| PUBLIC | 1\n| [NULL]\n|\n+\n---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\nCopy\nLanguage\n:\nEnglish\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas", "language": "en"}}